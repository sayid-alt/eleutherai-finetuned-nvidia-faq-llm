{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayid-alt/eleutherai-finetuned-nvidia-faq-llm/blob/main/training/training_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "id": "a4af3420-ff0e-472b-8b44-7a495ddf76c3",
      "cell_type": "markdown",
      "source": [
        "## Working Space"
      ],
      "metadata": {
        "id": "a4af3420-ff0e-472b-8b44-7a495ddf76c3"
      }
    },
    {
      "id": "t72wDiBziayb",
      "cell_type": "code",
      "source": [
        "# @title **Install Libraries**\n",
        "!pip install transformers datasets accelerate -q"
      ],
      "metadata": {
        "id": "t72wDiBziayb",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T04:06:56.16952Z",
          "iopub.execute_input": "2025-04-06T04:06:56.169817Z",
          "iopub.status.idle": "2025-04-06T04:07:00.436912Z",
          "shell.execute_reply.started": "2025-04-06T04:06:56.169776Z",
          "shell.execute_reply": "2025-04-06T04:07:00.436055Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "U6k7nB821ta0",
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "from pprint import pprint\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
        "\n",
        "from datasets import load_dataset\n",
        "import logging\n",
        "import torch\n",
        "import wandb\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n"
      ],
      "metadata": {
        "id": "U6k7nB821ta0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T04:07:00.437918Z",
          "iopub.execute_input": "2025-04-06T04:07:00.438135Z",
          "iopub.status.idle": "2025-04-06T04:07:24.915311Z",
          "shell.execute_reply.started": "2025-04-06T04:07:00.438116Z",
          "shell.execute_reply": "2025-04-06T04:07:24.914632Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "656681ff-6b88-4560-ba76-6980667da173",
      "cell_type": "code",
      "source": [
        "\n",
        "#@title **Load Pretrained Model**\n",
        "pretrained_model = 'EleutherAI/pythia-1b'\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(pretrained_model)\n",
        "\n",
        "dataset_hf_name = f\"nvidia-faq-{pretrained_model.split('/')[0].lower()}-fine-tuned\"\n",
        "\n",
        "# @title Setup Training\n",
        "model_finetuned_name = f\"{pretrained_model.split('/')[0]}-{pretrained_model.split('/')[1]}-finetuned-nvidia-faq\"\n",
        "output_dir = model_finetuned_name\n",
        "\n",
        "print(f'Finetuned Model Name: {model_finetuned_name}')\n",
        "print(f'dataset hf Name: {dataset_hf_name}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T04:07:24.916825Z",
          "iopub.execute_input": "2025-04-06T04:07:24.917112Z",
          "iopub.status.idle": "2025-04-06T04:07:35.673546Z",
          "shell.execute_reply.started": "2025-04-06T04:07:24.917083Z",
          "shell.execute_reply": "2025-04-06T04:07:35.672753Z"
        },
        "id": "656681ff-6b88-4560-ba76-6980667da173"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "af1edd38-9e1f-42ed-80e9-5c6ef6d86327",
      "cell_type": "code",
      "source": [
        "# @title **Logging To Hugging Face**\n",
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# login to hugging face\n",
        "notebook_login()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T04:07:35.674537Z",
          "iopub.execute_input": "2025-04-06T04:07:35.674762Z",
          "iopub.status.idle": "2025-04-06T04:07:39.664085Z",
          "shell.execute_reply.started": "2025-04-06T04:07:35.674743Z",
          "shell.execute_reply": "2025-04-06T04:07:39.662997Z"
        },
        "id": "af1edd38-9e1f-42ed-80e9-5c6ef6d86327"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5KRpRLxPtIlt",
      "cell_type": "code",
      "source": [
        "# @title **Load Data**\n",
        "def load_nvidia_faq_data(url, zip_path='nvidia_faq.zip', extract_dir='nvidia_faq'):\n",
        "    # Download the ZIP file from the URL\n",
        "    response = requests.get(url)\n",
        "    with open(zip_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {zip_path}\")\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"Extracted to {extract_dir}\")\n",
        "\n",
        "    # Find the CSV file inside the extracted folder\n",
        "    csv_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')]\n",
        "    if not csv_files:\n",
        "        raise FileNotFoundError(\"No CSV file found in the extracted content.\")\n",
        "\n",
        "    # Load the first CSV file found\n",
        "    csv_path = os.path.join(extract_dir, csv_files[0])\n",
        "    data = pd.read_csv(csv_path)\n",
        "    print(f\"Loaded data from {csv_path}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# URL to the Nvidia FAQ zip file (replace with the actual URL)\n",
        "url = 'https://github.com/sayid-alt/eleutherai-finetuned-nvidia-faq-llm/raw/main/datasets/NvidiaDocumentationQandApairs.zip'\n",
        "\n",
        "dataset = load_nvidia_faq_data(url)\n",
        "dataset = dataset[['question', 'answer']]\n",
        "display(dataset)"
      ],
      "metadata": {
        "id": "5KRpRLxPtIlt",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:58:41.767285Z",
          "iopub.execute_input": "2025-04-05T22:58:41.767615Z",
          "iopub.status.idle": "2025-04-05T22:58:42.288401Z",
          "shell.execute_reply.started": "2025-04-05T22:58:41.767588Z",
          "shell.execute_reply": "2025-04-05T22:58:42.287567Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "86p_8aFFyzZj",
      "cell_type": "markdown",
      "source": [
        "### **Data Preparation**"
      ],
      "metadata": {
        "id": "86p_8aFFyzZj"
      }
    },
    {
      "id": "lWdwY526nmVk",
      "cell_type": "code",
      "source": [
        "# @title **Preparing Finetuning Dataset**\n",
        "# prompt template\n",
        "prompt_template = \"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\"\"\"\n",
        "\n",
        "# array for storing question answer data\n",
        "finetuning_dataset = []\n",
        "for i in range(len(dataset)):\n",
        "  question = dataset.iloc[i]['question']\n",
        "  answer = dataset.iloc[i]['answer']\n",
        "  text_with_prompt_template = prompt_template.format(question=question)\n",
        "  finetuning_dataset.append({\n",
        "      \"question\": text_with_prompt_template,\n",
        "      \"answer\": answer\n",
        "  })\n",
        "\n",
        "finetuning_dataset = Dataset.from_list(finetuning_dataset)\n",
        "finetuning_dataset"
      ],
      "metadata": {
        "id": "lWdwY526nmVk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:58:44.061835Z",
          "iopub.execute_input": "2025-04-05T22:58:44.06221Z",
          "iopub.status.idle": "2025-04-05T22:58:44.419272Z",
          "shell.execute_reply.started": "2025-04-05T22:58:44.062177Z",
          "shell.execute_reply": "2025-04-05T22:58:44.418543Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "30yehFef3s8l",
      "cell_type": "code",
      "source": [
        "sample_text = finetuning_dataset['question'][0] + finetuning_dataset['answer'][0]\n",
        "sample_tokenized = tokenizer(sample_text, return_tensors='pt')\n",
        "sample_tokenized['input_ids'][0]"
      ],
      "metadata": {
        "id": "30yehFef3s8l",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:45:05.329447Z",
          "iopub.execute_input": "2025-04-05T22:45:05.329827Z",
          "iopub.status.idle": "2025-04-05T22:45:05.369253Z",
          "shell.execute_reply.started": "2025-04-05T22:45:05.329796Z",
          "shell.execute_reply": "2025-04-05T22:45:05.368302Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "L82Sw4qP1yOd",
      "cell_type": "code",
      "source": [
        "# @title Tokenize Dataset\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.truncation_side = 'left'\n",
        "  tokenized_input = tokenizer(\n",
        "      text,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      max_length=512,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "\n",
        "  return tokenized_input\n",
        "\n",
        "\n",
        "# tokenize dataset\n",
        "tokenized_dataset = finetuning_dataset.map(\n",
        "    lambda x: tokenize_function(x),\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    drop_last_batch=True,\n",
        "    # remove_columns=['question', 'answer']\n",
        ")"
      ],
      "metadata": {
        "id": "L82Sw4qP1yOd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:51:26.548203Z",
          "iopub.execute_input": "2025-04-05T22:51:26.548522Z",
          "iopub.status.idle": "2025-04-05T22:51:41.038483Z",
          "shell.execute_reply.started": "2025-04-05T22:51:26.548498Z",
          "shell.execute_reply": "2025-04-05T22:51:41.037211Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "vMWlA2qT8VOu",
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
      ],
      "metadata": {
        "id": "vMWlA2qT8VOu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:51:41.039724Z",
          "iopub.execute_input": "2025-04-05T22:51:41.040087Z",
          "iopub.status.idle": "2025-04-05T22:51:51.34793Z",
          "shell.execute_reply.started": "2025-04-05T22:51:41.04006Z",
          "shell.execute_reply": "2025-04-05T22:51:51.346852Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "JU2CiJGX7RQU",
      "cell_type": "code",
      "source": [
        "# @title Split Dataset\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=25)\n",
        "train_dataset = split_dataset['train']\n",
        "test_dataset = split_dataset['test']\n",
        "\n",
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "JU2CiJGX7RQU",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:52:14.830073Z",
          "iopub.execute_input": "2025-04-05T22:52:14.830448Z",
          "iopub.status.idle": "2025-04-05T22:52:14.909877Z",
          "shell.execute_reply.started": "2025-04-05T22:52:14.830418Z",
          "shell.execute_reply": "2025-04-05T22:52:14.908674Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a61b2d55-ac1a-42bb-b5e8-f829db064ec6",
      "cell_type": "code",
      "source": [
        "# check if all size inputs are the same length\n",
        "len(train_dataset['input_ids'][5]) == len(train_dataset['input_ids'][10])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:52:15.068168Z",
          "iopub.execute_input": "2025-04-05T22:52:15.068528Z",
          "iopub.status.idle": "2025-04-05T22:52:17.64221Z",
          "shell.execute_reply.started": "2025-04-05T22:52:15.068496Z",
          "shell.execute_reply": "2025-04-05T22:52:17.641346Z"
        },
        "id": "a61b2d55-ac1a-42bb-b5e8-f829db064ec6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "34ae73c5-a112-4918-b891-28da72a681c9",
      "cell_type": "code",
      "source": [
        "example_encoded = train_dataset['input_ids'][0]\n",
        "example_decoded = tokenizer.decode(example_encoded, skip_special_tokens=True)\n",
        "\n",
        "print(example_encoded, '\\n', example_decoded)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:52:32.836417Z",
          "iopub.execute_input": "2025-04-05T22:52:32.836773Z",
          "iopub.status.idle": "2025-04-05T22:52:34.581985Z",
          "shell.execute_reply.started": "2025-04-05T22:52:32.836721Z",
          "shell.execute_reply": "2025-04-05T22:52:34.580971Z"
        },
        "id": "34ae73c5-a112-4918-b891-28da72a681c9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eVQE8QvH-4IX",
      "cell_type": "code",
      "source": [
        "# pushing\n",
        "split_dataset.push_to_hub(dataset_hf_name)"
      ],
      "metadata": {
        "id": "eVQE8QvH-4IX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:52:58.643186Z",
          "iopub.execute_input": "2025-04-05T22:52:58.643525Z",
          "iopub.status.idle": "2025-04-05T22:53:05.009263Z",
          "shell.execute_reply.started": "2025-04-05T22:52:58.6435Z",
          "shell.execute_reply": "2025-04-05T22:53:05.008167Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "zk8q3PE6BLcw",
      "cell_type": "markdown",
      "source": [
        "### Training Data"
      ],
      "metadata": {
        "id": "zk8q3PE6BLcw"
      }
    },
    {
      "id": "Ax99D1sfF4N-",
      "cell_type": "code",
      "source": [
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "Ax99D1sfF4N-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:58:52.366008Z",
          "iopub.execute_input": "2025-04-05T22:58:52.366424Z",
          "iopub.status.idle": "2025-04-05T22:58:58.146365Z",
          "shell.execute_reply.started": "2025-04-05T22:58:52.36639Z",
          "shell.execute_reply": "2025-04-05T22:58:58.145461Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e1a610ee-af56-45b1-8e86-04cfe3619c57",
      "cell_type": "code",
      "source": [
        "# # !wandb login\n",
        "\n",
        "# # Login to wandb using kaggle notebook\n",
        "# from kaggle_secrets import UserSecretsClient\n",
        "# user_secrets = UserSecretsClient()\n",
        "# secret_value_0 = user_secrets.get_secret(\"wandb_api_key\")"
      ],
      "metadata": {
        "id": "e1a610ee-af56-45b1-8e86-04cfe3619c57",
        "execution": {
          "iopub.status.busy": "2025-04-05T22:58:58.147763Z",
          "iopub.execute_input": "2025-04-05T22:58:58.14801Z",
          "iopub.status.idle": "2025-04-05T22:58:58.151567Z",
          "shell.execute_reply.started": "2025-04-05T22:58:58.14799Z",
          "shell.execute_reply": "2025-04-05T22:58:58.150571Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "96e32248-2936-4159-87e2-30d7ada796fd",
      "cell_type": "code",
      "source": [
        "%env WANDB_PROJECT=eleutherai-nvidia-faq-fine-tuned\n",
        "%env WANDB_WATCH=true\n",
        "%env WANDB_LOG_MODEL=end"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:58:58.152885Z",
          "iopub.execute_input": "2025-04-05T22:58:58.153085Z",
          "iopub.status.idle": "2025-04-05T22:58:58.171451Z",
          "shell.execute_reply.started": "2025-04-05T22:58:58.153068Z",
          "shell.execute_reply": "2025-04-05T22:58:58.170674Z"
        },
        "id": "96e32248-2936-4159-87e2-30d7ada796fd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "FLK5o-NECyoq",
      "cell_type": "code",
      "source": [
        "# @title Load Dataset\n",
        "dataset_path_hf = f'paacamo/{dataset_hf_name}'\n",
        "dataset = load_dataset(dataset_path_hf)\n",
        "\n",
        "train_dataset = dataset['train'].map(remove_columns=(['question', 'answer'])) #use this for deleted some columns\n",
        "test_dataset = dataset['test'].map(remove_columns=(['question', 'answer']))\n",
        "\n",
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "FLK5o-NECyoq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:58:58.172424Z",
          "iopub.execute_input": "2025-04-05T22:58:58.172884Z",
          "iopub.status.idle": "2025-04-05T22:59:01.918712Z",
          "shell.execute_reply.started": "2025-04-05T22:58:58.172854Z",
          "shell.execute_reply": "2025-04-05T22:59:01.917856Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4d6afd51-1bd6-4659-8a7f-09128187b842",
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:59:01.919799Z",
          "iopub.execute_input": "2025-04-05T22:59:01.920058Z",
          "iopub.status.idle": "2025-04-05T22:59:02.192299Z",
          "shell.execute_reply.started": "2025-04-05T22:59:01.920037Z",
          "shell.execute_reply": "2025-04-05T22:59:02.191231Z"
        },
        "id": "4d6afd51-1bd6-4659-8a7f-09128187b842"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "k3qlUstACKNm",
      "cell_type": "code",
      "source": [
        "device_count = torch.cuda.device_count()\n",
        "print(device_count)\n",
        "if device_count > 0:\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "base_model.to(device)"
      ],
      "metadata": {
        "id": "k3qlUstACKNm",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T22:59:02.193858Z",
          "iopub.execute_input": "2025-04-05T22:59:02.194226Z",
          "iopub.status.idle": "2025-04-05T22:59:03.6886Z",
          "shell.execute_reply.started": "2025-04-05T22:59:02.194187Z",
          "shell.execute_reply": "2025-04-05T22:59:03.687854Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "CpAPWThUDNKc",
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    # Learning Rate\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    remove_unused_columns=False,\n",
        "\n",
        "    # Epochs\n",
        "    num_train_epochs=2,\n",
        "\n",
        "    # Batch Trainig Size\n",
        "    per_device_train_batch_size=8,\n",
        "\n",
        "    output_dir=output_dir,\n",
        "\n",
        "    # max_steps=10,\n",
        "\n",
        "    # Other arguments\n",
        "    overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "    disable_tqdm=False, # Disable progress bars\n",
        "    eval_steps=100, # Number of update steps between two evaluations\n",
        "    save_steps=100, # After # steps model is saved\n",
        "    warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
        "    per_device_eval_batch_size=8, # Batch size for evaluation\n",
        "    save_strategy='steps',\n",
        "    eval_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,\n",
        "    optim=\"adafactor\",\n",
        "    gradient_accumulation_steps = 1,\n",
        "    gradient_checkpointing=False,\n",
        "\n",
        "    # Parameters for early stopping\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=1,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    push_to_hub=True,\n",
        "    report_to='wandb',\n",
        "    run_name=model_finetuned_name\n",
        ")"
      ],
      "metadata": {
        "id": "CpAPWThUDNKc",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T23:00:59.342809Z",
          "iopub.execute_input": "2025-04-05T23:00:59.343102Z",
          "iopub.status.idle": "2025-04-05T23:00:59.371481Z",
          "shell.execute_reply.started": "2025-04-05T23:00:59.343081Z",
          "shell.execute_reply": "2025-04-05T23:00:59.370825Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "Nu1omLRSGeEQ",
      "cell_type": "code",
      "source": [
        "# @title Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "Nu1omLRSGeEQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T23:01:01.012147Z",
          "iopub.execute_input": "2025-04-05T23:01:01.012513Z",
          "iopub.status.idle": "2025-04-05T23:01:01.161502Z",
          "shell.execute_reply.started": "2025-04-05T23:01:01.012484Z",
          "shell.execute_reply": "2025-04-05T23:01:01.160634Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "kYHHhxEQC-GS",
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "kYHHhxEQC-GS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T23:01:01.294063Z",
          "iopub.execute_input": "2025-04-05T23:01:01.294321Z",
          "iopub.status.idle": "2025-04-06T01:23:45.290293Z",
          "shell.execute_reply.started": "2025-04-05T23:01:01.2943Z",
          "shell.execute_reply": "2025-04-06T01:23:45.271482Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "YOXWAUCPuMh1",
      "cell_type": "code",
      "source": [
        "# @title Save model\n",
        "save_dir = f'{output_dir}/final'\n",
        "trainer.save_model(save_dir)\n",
        "print(f'model saved to {save_dir}')"
      ],
      "metadata": {
        "id": "YOXWAUCPuMh1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T01:29:29.641162Z",
          "iopub.status.idle": "2025-04-04T01:29:29.64145Z",
          "shell.execute_reply": "2025-04-04T01:29:29.641331Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3deb64cd-f9d4-449b-8b95-edcb325cfea9",
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-04T01:29:29.642269Z",
          "iopub.status.idle": "2025-04-04T01:29:29.642643Z",
          "shell.execute_reply": "2025-04-04T01:29:29.642446Z"
        },
        "id": "3deb64cd-f9d4-449b-8b95-edcb325cfea9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7EOjcmLlvLkX",
      "cell_type": "markdown",
      "source": [
        "## **Inference & Evaluation**"
      ],
      "metadata": {
        "id": "7EOjcmLlvLkX"
      }
    },
    {
      "id": "dPIuOWqXtM4g",
      "cell_type": "code",
      "source": [
        "# @title load Fine-Tuned Model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "finetuned_model_name = f'paacamo/{output_dir}'\n",
        "\n",
        "finetuned_model = AutoModelForCausalLM.from_pretrained(finetuned_model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(finetuned_model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "finetuned_model.to(device)"
      ],
      "metadata": {
        "id": "dPIuOWqXtM4g",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T04:29:11.952654Z",
          "iopub.execute_input": "2025-04-06T04:29:11.953127Z",
          "iopub.status.idle": "2025-04-06T04:29:14.658519Z",
          "shell.execute_reply.started": "2025-04-06T04:29:11.953089Z",
          "shell.execute_reply": "2025-04-06T04:29:14.657607Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ZrIATp_eHF3w",
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_token=1000, max_output_token=500):\n",
        "  # Tokenize\n",
        "  tokenizer.truncation_side = 'left'\n",
        "  input_ids = tokenizer.encode(\n",
        "      text,\n",
        "      return_tensors='pt',\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=max_input_token\n",
        "  )\n",
        "\n",
        "  # generate\n",
        "  device = model.device\n",
        "  output_ids = finetuned_model.generate(\n",
        "      input_ids=input_ids.to(device),\n",
        "      max_length=max_output_token\n",
        "  )\n",
        "\n",
        "  # decode\n",
        "  decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "   # Strip the prompt\n",
        "  generated_text_answer = decoded_output[len(text):]\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "ZrIATp_eHF3w",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T04:29:57.732181Z",
          "iopub.execute_input": "2025-04-06T04:29:57.732476Z",
          "iopub.status.idle": "2025-04-06T04:29:57.737919Z",
          "shell.execute_reply.started": "2025-04-06T04:29:57.732455Z",
          "shell.execute_reply": "2025-04-06T04:29:57.736793Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1qEkTIvZ09Ug",
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "\n",
        "dataset_infer = load_dataset(\"paacamo/\"+dataset_hf_name, split='test')\n",
        "\n",
        "text = dataset_infer['question'][90]\n",
        "answer = dataset_infer['answer'][90]\n",
        "\n",
        "print(f'question: {text}')\n",
        "predictions = {\n",
        "    'answer': answer,\n",
        "    'prediction': inference(text, finetuned_model, tokenizer)\n",
        "}\n",
        "\n",
        "pprint(predictions)"
      ],
      "metadata": {
        "id": "1qEkTIvZ09Ug",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T04:32:26.19728Z",
          "iopub.execute_input": "2025-04-06T04:32:26.197615Z",
          "iopub.status.idle": "2025-04-06T04:32:28.250018Z",
          "shell.execute_reply.started": "2025-04-06T04:32:26.197586Z",
          "shell.execute_reply": "2025-04-06T04:32:28.24929Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "BK2ZlqtRSU-m",
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=finetuned_model_name)\n",
        "pipe(\"what is the purpose of using CUDA rather than cpu?\")"
      ],
      "metadata": {
        "id": "BK2ZlqtRSU-m",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T04:32:53.131435Z",
          "iopub.execute_input": "2025-04-06T04:32:53.131749Z",
          "iopub.status.idle": "2025-04-06T04:32:55.308098Z",
          "shell.execute_reply.started": "2025-04-06T04:32:53.131725Z",
          "shell.execute_reply": "2025-04-06T04:32:55.307209Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interface"
      ],
      "metadata": {
        "id": "fJsWUazLbd80"
      },
      "id": "fJsWUazLbd80"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio langchain langchain-core langchain langchain_huggingface langchain-community langchain_google_genai python-dotenv -q"
      ],
      "metadata": {
        "id": "Uhh_CAWmbgYQ"
      },
      "id": "Uhh_CAWmbgYQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "c1fb239a-1c81-4476-9009-d87abadf9506",
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "# Feel free to add new code block as needed\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "def chatbot(question):\n",
        "    \"\"\"\n",
        "    This function takes a question as input and returns the chatbot's response.\n",
        "    \"\"\"\n",
        "    pipe = pipeline('text-generation', model=finetuned_model_name)\n",
        "    response = pipe(question)[0]['generated_text']\n",
        "    return response\n",
        "\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"Simple Chatbot with Langchain and Gradio\",\n",
        "    description=\"Ask me anything!\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "id": "c1fb239a-1c81-4476-9009-d87abadf9506"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}